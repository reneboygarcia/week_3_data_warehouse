{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "from prefect import task, flow\n",
    "from prefect_gcp.cloud_storage import GcsBucket\n",
    "\n",
    "print(\"Setup Complete\")\n",
    "\n",
    "\n",
    "# Deployment 1\n",
    "# Fetch the data from Github url\n",
    "@task(log_prints=True, name=\"fetch-file-dataset_url\")\n",
    "def fetch(dataset_url: str):\n",
    "    filename, _ = urllib.request.urlretrieve(dataset_url)\n",
    "    return filename\n",
    "\n",
    "\n",
    "# Read and tweak to fix the dtypes of pick-up and drop-off\n",
    "@task(log_prints=True, name=\"read-and-tweak-df\")\n",
    "def read_tweak_df(src: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(src, parse_dates=[2, 3], compression=\"gzip\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# Write DataFrame to a specific folder after tweaking the DataFrame\n",
    "@task(log_prints=True, name=\"write-to-local-file\")\n",
    "def write_local(df: pd.DataFrame, year: int, dataset_file: str) -> Path:\n",
    "    directory = Path(f\"{year}\")\n",
    "    path_name = directory / f\"{dataset_file}.parquet\"\n",
    "    try:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        df.to_parquet(path_name, compression=\"gzip\", index=False)\n",
    "    except OSError as error:\n",
    "        print(error)\n",
    "    return path_name\n",
    "\n",
    "\n",
    "# https://app.prefect.cloud/account/975bd9ed-5aef-4c8a-a413-23073fef3acb/workspace/a711abba-f5ae-4b00-9315-34f92f089b77/blocks/catalog\n",
    "# Upload local csv.gz file to GCS\n",
    "@task(log_prints=True)\n",
    "def write_gcs(path: Path) -> None:\n",
    "    gcs_block = GcsBucket.load(\"prefect-gcs-block-ny-taxi\")\n",
    "    gcs_block.upload_from_path(from_path=path, to_path=path)\n",
    "    print(\"Loaded data to GCS...Hooray!\")\n",
    "    return\n",
    "\n",
    "\n",
    "# Delete file after uploads\n",
    "@task(log_prints=True, name=\"deduplicate-local-data\")\n",
    "def deduplicate(path: Path) -> None:\n",
    "    try:\n",
    "        os.remove(path)\n",
    "        os.rmdir(\n",
    "            path,\n",
    "        )\n",
    "        print(\"Successfully deleted directory and local files...hep hep hooray\")\n",
    "    except OSError as error:\n",
    "        print(f\"The system cannot find the file specified: {error}\")\n",
    "    return\n",
    "\n",
    "\n",
    "@flow(log_prints=True, name=\"etl-web-gcs\")\n",
    "def etl_web_gcs(year: int, month: int):\n",
    "    # Parameters\n",
    "    year = 2019\n",
    "    month = 5\n",
    "    dataset_file = f\"fhv_tripdata_{year}-{month:02}\"\n",
    "    dataset_url = f\"https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/{dataset_file}.csv.gz\"\n",
    "\n",
    "    # Execution\n",
    "    # Fetch the data from Github url\n",
    "    source = fetch(dataset_url)\n",
    "\n",
    "    # no choice but to tweak the data types for us to upload this to BQ\n",
    "    # Read and tweak to fix the dytpes of pick-up and drop-off\n",
    "    df_tweak = read_tweak_df(source)\n",
    "\n",
    "    # write df to local\n",
    "    path_local = write_local(df_tweak, year, dataset_file)\n",
    "\n",
    "    # Upload dataset from local to gcs\n",
    "    write_gcs(path_local)\n",
    "\n",
    "    # Delete file after uploads\n",
    "    deduplicate(path_local)\n",
    "\n",
    "    # Next step is to create a separate deployment from GCS to BigQuery\n",
    "\n",
    "\n",
    "# Create Parent flow to loop\n",
    "@flow(name=\"etl-parent-local-gcs\")\n",
    "def etl_parent_web_gcs(year: int, months: list):\n",
    "    year = 2019\n",
    "    months = [2]\n",
    "\n",
    "    for month in months:\n",
    "        etl_web_gcs(year, month)\n",
    "\n",
    "\n",
    "# Run main\n",
    "if __name__ == \"__main__\":\n",
    "    etl_parent_web_gcs(year=2019, months=[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "from prefect_gcp import GcpCredentials\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# Deployment 2\n",
    "\n",
    "\n",
    "\n",
    "# Upload data from GCS to BigQuery\n",
    "@flow(log_prints=True, name=\"etl-gcs-to-bq\")\n",
    "def etl_gcs_to_bq(year: int, month: int):\n",
    "\n",
    "    gcp_creds_block = GcpCredentials.load(\"prefect-gcs-2023-creds\")\n",
    "    gcp_creds = gcp_creds_block.get_credentials_from_service_account()\n",
    "    client = bigquery.Client(credentials=gcp_creds)\n",
    "    table_id = \"dtc-de-2023.ny_taxi.ny_taxi_tripdata_2019\"\n",
    "\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        source_format=bigquery.SourceFormat.CSV,\n",
    "        schema=[\n",
    "            bigquery.SchemaField(\"dispatching_base_num\", \"STRING\", mode=\"NULLABLE\"),\n",
    "            bigquery.SchemaField(\"pickup_datetime\", \"TIMESTAMP\", mode=\"NULLABLE\"),\n",
    "            bigquery.SchemaField(\"dropOff_datetime\", \"TIMESTAMP\", mode=\"NULLABLE\"),\n",
    "            bigquery.SchemaField(\"PUlocationID\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "            bigquery.SchemaField(\"DOlocationID\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "            bigquery.SchemaField(\n",
    "                \"SR_Flag\",\n",
    "                \"FLOAT\",\n",
    "                mode=\"NULLABLE\",\n",
    "            ),\n",
    "            bigquery.SchemaField(\"Affiliated_base_number\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        ],\n",
    "    )\n",
    "    uri = f\"gs://ny_taxi_bucket_de_2023/{year}/fhv_tripdata_{year}-{month:02}.csv.gz\"\n",
    "\n",
    "    load_job = client.load_table_from_uri(\n",
    "        uri, table_id, job_config=job_config\n",
    "    )  # Make an API request.\n",
    "\n",
    "    load_job.result()  # Waits for the job to complete.\n",
    "\n",
    "    destination_table = client.get_table(table_id)\n",
    "    print(f\"Loaded {destination_table.num_rows} rows.\")\n",
    "\n",
    "\n",
    "# Parent flow ETL\n",
    "@flow(log_prints=True, name=\"etl-parent-to-bq\")\n",
    "def etl_parent_bq_flow(\n",
    "    year: int = 2019, months: list[int] = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "):\n",
    "    for month in months:\n",
    "        etl_gcs_to_bq(year, month)\n",
    "\n",
    "\n",
    "# run main\n",
    "if __name__ == \"__main__\":\n",
    "    year = 2019\n",
    "    months = [3]\n",
    "\n",
    "    etl_parent_bq_flow(year, months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dispatching_base_num</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropOff_datetime</th>\n",
       "      <th>PUlocationID</th>\n",
       "      <th>DOlocationID</th>\n",
       "      <th>SR_Flag</th>\n",
       "      <th>Affiliated_base_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B00037</td>\n",
       "      <td>2019-02-01 00:08:44</td>\n",
       "      <td>2019-02-01 00:23:35</td>\n",
       "      <td>264.0</td>\n",
       "      <td>265.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B00037</td>\n",
       "      <td>2019-02-01 00:27:51</td>\n",
       "      <td>2019-02-01 00:32:54</td>\n",
       "      <td>264.0</td>\n",
       "      <td>265.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B00037</td>\n",
       "      <td>2019-02-01 00:18:30</td>\n",
       "      <td>2019-02-01 00:25:45</td>\n",
       "      <td>264.0</td>\n",
       "      <td>265.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B00037</td>\n",
       "      <td>2019-02-01 00:43:15</td>\n",
       "      <td>2019-02-01 00:48:29</td>\n",
       "      <td>264.0</td>\n",
       "      <td>265.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B00037</td>\n",
       "      <td>2019-02-01 00:01:45</td>\n",
       "      <td>2019-02-01 00:09:13</td>\n",
       "      <td>264.0</td>\n",
       "      <td>265.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dispatching_base_num      pickup_datetime     dropOff_datetime  \\\n",
       "0               B00037  2019-02-01 00:08:44  2019-02-01 00:23:35   \n",
       "1               B00037  2019-02-01 00:27:51  2019-02-01 00:32:54   \n",
       "2               B00037  2019-02-01 00:18:30  2019-02-01 00:25:45   \n",
       "3               B00037  2019-02-01 00:43:15  2019-02-01 00:48:29   \n",
       "4               B00037  2019-02-01 00:01:45  2019-02-01 00:09:13   \n",
       "\n",
       "   PUlocationID  DOlocationID  SR_Flag Affiliated_base_number  \n",
       "0         264.0         265.0      NaN                 B00037  \n",
       "1         264.0         265.0      NaN                 B00037  \n",
       "2         264.0         265.0      NaN                 B00037  \n",
       "3         264.0         265.0      NaN                 B00037  \n",
       "4         264.0         265.0      NaN                 B00037  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src = \"/Users/reneboygarcia/Downloads/fhv_tripdata_2019-02.csv\"\n",
    "df = pd.read_csv(src)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1707649 entries, 0 to 1707648\n",
      "Data columns (total 7 columns):\n",
      " #   Column                  Dtype  \n",
      "---  ------                  -----  \n",
      " 0   dispatching_base_num    object \n",
      " 1   pickup_datetime         object \n",
      " 2   dropOff_datetime        object \n",
      " 3   PUlocationID            float64\n",
      " 4   DOlocationID            float64\n",
      " 5   SR_Flag                 float64\n",
      " 6   Affiliated_base_number  object \n",
      "dtypes: float64(3), object(4)\n",
      "memory usage: 91.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c6e4e9f98eb68ad3b7c296f83d20e6de614cb42e90992a65aa266555a3137d0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
